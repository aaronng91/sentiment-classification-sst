{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment Classification on Stanford Sentiment Treebank",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A43-Jjo1ySkk",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification on Stanford Sentiment Treebank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIFl11dIyQUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import vocab\n",
        "from torchtext.vocab import Vectors\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byixbpx4yQUf",
        "colab_type": "text"
      },
      "source": [
        "Download SST dataset and load into notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VN4ALzOgRu6u",
        "colab": {}
      },
      "source": [
        "! `curl -fsS https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip -o /tmp/trainDevTestTrees_PTB.zip`\n",
        "! `unzip -q -o -d /tmp /tmp/trainDevTestTrees_PTB.zip`\n",
        "! `rm -f /tmp/trainDevTestTrees_PTB.zip`"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OJRhwJrtRu6x",
        "colab": {}
      },
      "source": [
        "def loadsst(path):\n",
        "    xs = list()\n",
        "    ys = list() \n",
        "    with open(path) as file:\n",
        "        # Quick, dirty, and improper S-expression parsing.\n",
        "        for i , line in enumerate(file):\n",
        "            soup = line.split()\n",
        "            ys.append(int(soup[0].lstrip(\"(\")))\n",
        "            tokens = list()\n",
        "            for chunk in soup[1:-1]:\n",
        "                if chunk[-1] == \")\":\n",
        "                    token = chunk.rstrip(')')\n",
        "                    tokens.append(token)\n",
        "            xs.append(tokens)\n",
        "    return xs, ys\n",
        "        \n",
        "ssttrainxs, ssttrainys = loadsst(\"/tmp/trees/train.txt\")\n",
        "sstvalidxs, sstvalidys = loadsst(\"/tmp/trees/dev.txt\")\n",
        "ssttestxs, ssttestys   = loadsst(\"/tmp/trees/test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9FeiHkx8Ru6z",
        "colab": {}
      },
      "source": [
        "trainx = pd.DataFrame.from_records([[' '.join(x)] for x in ssttrainxs])\n",
        "trainy = pd.DataFrame(ssttrainys)\n",
        "validx = pd.DataFrame.from_records([[' '.join(x)] for x in sstvalidxs])\n",
        "validy = pd.DataFrame(sstvalidys)\n",
        "testx = pd.DataFrame.from_records([[' '.join(x)] for x in ssttestxs])\n",
        "testy = pd.DataFrame(ssttestys)\n",
        "train = pd.concat([trainx, trainy], axis=1)\n",
        "valid = pd.concat([validx, validy], axis=1)\n",
        "test = pd.concat([testx, testy], axis=1)\n",
        "train.to_csv('/tmp/train.csv', index=False)\n",
        "valid.to_csv('/tmp/valid.csv', index=False)\n",
        "test.to_csv('/tmp/test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNKPmaHuyQU3",
        "colab_type": "text"
      },
      "source": [
        "Use PyTorch's TorchText library to load the training, validation and testing dataset. Here, we define our fields, where TEXT are our reviews and LABEL are our sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7vO2qAZ8Ru61",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(include_lengths=True)\n",
        "LABEL = data.Field(\n",
        "    use_vocab=False,\n",
        "    sequential=False,\n",
        "    pad_token=None,\n",
        "    unk_token=None\n",
        ")\n",
        "fields = [('Text', TEXT), ('Label', LABEL)]\n",
        "\n",
        "trainds, valds, testds = data.TabularDataset.splits(path='/tmp', \n",
        "                                            format='csv', \n",
        "                                            train='train.csv', \n",
        "                                            validation='valid.csv', \n",
        "                                            test='test.csv',\n",
        "                                            fields=fields, \n",
        "                                            skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETyNSPVbyQU6",
        "colab_type": "text"
      },
      "source": [
        "Download embeddings and load it into notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bwJ9x1OlRu65",
        "colab": {}
      },
      "source": [
        "! `curl -fsS https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz -o /tmp/GoogleNews-vectors-negative300.bin.gz`"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w8yJY5_VRu69",
        "outputId": "1716ee99-0e89-4c64-d678-9c578d00e3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format('/tmp/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "model.save_word2vec_format('/tmp/GoogleNews-vectors-negative300.txt', binary=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VrUrcGqyQU_",
        "colab_type": "text"
      },
      "source": [
        "We use build_vocab function in TorchText to build the vocabulary for our dataset using the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jt_HaSLGUq3y",
        "outputId": "280dd060-8bd6-4e55-a9be-9e70a453f288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TEXT.build_vocab(trainds, vectors=vocab.Vectors('GoogleNews-vectors-negative300.txt', '/tmp'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3000000 [00:00<?, ?it/s]Skipping token b'3000000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 2999813/3000000 [06:43<00:00, 8197.36it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9Gurpf8yQVE",
        "colab_type": "text"
      },
      "source": [
        "We can inspect our vocabulary and observe that we have a 18254 x 300 dictionary, where each word is represented by a vector of size 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cp-5yp-jebKI",
        "outputId": "ea0b853d-8813-427d-970f-e601c773290e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT.vocab.vectors.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18254, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFHWGZzdaL8P",
        "outputId": "5a6e9677-6a86-4b92-f266-15872731d72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "TEXT.vocab.vectors[TEXT.vocab.stoi['the']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0801,  0.1050,  0.0498,  0.0535, -0.0674, -0.1206,  0.0352, -0.1187,\n",
              "         0.0439,  0.0302, -0.0569, -0.0762,  0.0129,  0.0498, -0.0850, -0.0635,\n",
              "         0.0063, -0.0432,  0.0203,  0.0133, -0.0195,  0.0928, -0.1719, -0.0013,\n",
              "         0.0654,  0.0583, -0.0825,  0.0859, -0.0032,  0.0586, -0.0349, -0.0123,\n",
              "        -0.0481, -0.0030,  0.0564,  0.0150, -0.0723, -0.0522,  0.0967,  0.0430,\n",
              "        -0.0354, -0.0732,  0.0327, -0.0618,  0.0079,  0.0036, -0.0088,  0.0391,\n",
              "         0.0383,  0.0444,  0.0698,  0.0126, -0.0045, -0.0332, -0.0427,  0.0977,\n",
              "        -0.0216, -0.0378,  0.0119, -0.0139, -0.1133,  0.0933, -0.0393, -0.1162,\n",
              "         0.0233, -0.0160,  0.0264,  0.1074, -0.0047,  0.0962,  0.0280, -0.0540,\n",
              "         0.0854, -0.0369, -0.0203, -0.0854,  0.1250,  0.1445,  0.0267,  0.1504,\n",
              "         0.0527, -0.1865,  0.0815, -0.0106, -0.0374, -0.0732, -0.0752,  0.0361,\n",
              "        -0.1318,  0.0062,  0.0508,  0.0452,  0.0101, -0.1504, -0.0601,  0.0576,\n",
              "        -0.0069,  0.0159, -0.0214,  0.1035, -0.0003, -0.0469, -0.0164, -0.0786,\n",
              "        -0.0693,  0.0164, -0.0315, -0.0137, -0.0366, -0.0889, -0.0481, -0.0132,\n",
              "        -0.0718,  0.0059, -0.0461,  0.0398,  0.1006, -0.0493,  0.0757,  0.0388,\n",
              "        -0.1670, -0.0962, -0.1011,  0.0291, -0.0579, -0.0193, -0.0430, -0.0840,\n",
              "        -0.0199,  0.0515,  0.0085, -0.0361, -0.1494, -0.0186, -0.0364, -0.0767,\n",
              "        -0.0396, -0.0615, -0.0200,  0.0415,  0.0369, -0.0723,  0.0059, -0.0630,\n",
              "         0.0074, -0.0159,  0.0161, -0.0145,  0.0077,  0.1011, -0.0056,  0.0143,\n",
              "        -0.0762,  0.0564, -0.0129,  0.0306, -0.0249, -0.0986,  0.0325, -0.0281,\n",
              "        -0.0811,  0.0206,  0.0161, -0.0420, -0.0349, -0.0376,  0.0549,  0.0137,\n",
              "         0.0269, -0.0586, -0.0718, -0.1201, -0.0228, -0.1641, -0.0036, -0.0598,\n",
              "         0.0708, -0.0771,  0.0518, -0.0430, -0.0483,  0.0300, -0.0659, -0.0317,\n",
              "        -0.0488, -0.0349,  0.0588, -0.0146,  0.1807,  0.0569,  0.0525,  0.0579,\n",
              "         0.1167,  0.0520, -0.0535,  0.0187, -0.0156,  0.0058, -0.0732, -0.1162,\n",
              "         0.0405,  0.0625, -0.0432,  0.0106,  0.0217,  0.0425,  0.0327,  0.0442,\n",
              "         0.0576,  0.0261, -0.0183, -0.0270, -0.0067,  0.0051, -0.1162,  0.0036,\n",
              "         0.0576, -0.0596, -0.0884,  0.0135,  0.0454, -0.0464, -0.0177, -0.0625,\n",
              "         0.0344, -0.0242,  0.0309,  0.0957,  0.0796,  0.0393,  0.0280, -0.0859,\n",
              "         0.0811,  0.0664, -0.0004, -0.0693,  0.0359, -0.0342,  0.0449, -0.0077,\n",
              "        -0.0074, -0.0476,  0.0140, -0.0996,  0.0247, -0.0996,  0.1147,  0.0317,\n",
              "         0.0221,  0.0723,  0.0369,  0.0256,  0.0137, -0.0273,  0.0059, -0.0674,\n",
              "         0.0505, -0.0283, -0.0452, -0.0173,  0.0211,  0.0352, -0.0430,  0.0664,\n",
              "         0.1221,  0.1235,  0.0040,  0.0452, -0.0186,  0.0483,  0.0452,  0.0869,\n",
              "         0.0294,  0.0376,  0.0344, -0.0737, -0.0403, -0.1465, -0.0244, -0.0195,\n",
              "         0.0066, -0.0018, -0.0109,  0.0933,  0.0654,  0.0184, -0.0933, -0.0157,\n",
              "        -0.0713, -0.0894, -0.0713, -0.0302, -0.0130,  0.0164, -0.0183,  0.0148,\n",
              "         0.0050,  0.0037,  0.0476, -0.0688])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mfR8zt3yQVP",
        "colab_type": "text"
      },
      "source": [
        "Using TorchText's BucketIterator, we can load our training and validation set into a DataLoader, which helps in batching of our data and padding of sequences to match the longest sequence in the batch. Sequences of similar lengths are batched together, so we don't end up with too much padding.\n",
        "\n",
        "Here, we show an example of a batch of size 3 being created, where each column in the batch is a sequence converted into a vector of vocabulary indexes, with padding if necessary, and the corresponding labels in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mMRj2PAvabW2",
        "colab": {}
      },
      "source": [
        "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, testds),\n",
        "                                            batch_sizes=(3,3),\n",
        "                                            sort_key=lambda x: len(x.Text),\n",
        "                                            device=None,\n",
        "                                            sort_within_batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8_Fi_OaTeiEL",
        "outputId": "27c2aa8d-a888-4886-c7c8-926c98068493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "batch = next(iter(traindl))\n",
        "print(batch.Text)\n",
        "print(batch.Label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[ 6442,  2921,    27],\n",
            "        [   14,     3,  1744],\n",
            "        [   39,   331,   155],\n",
            "        [    6,     9,     6],\n",
            "        [    3,   119,     3],\n",
            "        [  238,  4012,   147],\n",
            "        [    8,     2,   124],\n",
            "        [   58,    71,    37],\n",
            "        [  537,    96,     7],\n",
            "        [   19,     3,   738],\n",
            "        [ 1316,    19,   233],\n",
            "        [    2,   469,     4],\n",
            "        [ 2034,     9,   425],\n",
            "        [   20,   400,     5],\n",
            "        [ 1589,   588,   801],\n",
            "        [ 3465,     2,     6],\n",
            "        [   18,  5656,   150],\n",
            "        [   12,    69,   227],\n",
            "        [  195,     2, 13978],\n",
            "        [   12,     4,    12],\n",
            "        [ 3253,     3,    57],\n",
            "        [   23,   342,     1],\n",
            "        [    7,   159,     1],\n",
            "        [ 2390,    31,     1],\n",
            "        [11524,  6192,     1],\n",
            "        [   71,  4569,     1],\n",
            "        [    5,     1,     1],\n",
            "        [   19,     1,     1],\n",
            "        [   63,     1,     1],\n",
            "        [   26,     1,     1]]), tensor([30, 26, 21]))\n",
            "tensor([4, 4, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5G8-TIyQVV",
        "colab_type": "text"
      },
      "source": [
        "Here, we create a wrapper class around the BucketIterator, since it returns a Batch object instead of the text and labels in each batch. This will help when accessing the text and labels in each batch directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuQAFa8wq2gK",
        "colab": {}
      },
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, dl, x_field, y_field):\n",
        "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            X = getattr(batch, self.x_field)\n",
        "            y = getattr(batch, self.y_field)\n",
        "            yield (X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-sawGndyQVY",
        "colab_type": "text"
      },
      "source": [
        "Definition of our Sentiment Classification model, using RNN, followed by a MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPyLn5kRt7RX",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, embed_dim, pretrained_vec, \n",
        "        num_hidden_rnn, num_rnn_layers, \n",
        "        mlp_layers, num_classes,\n",
        "        rec_unit_type, train_embeddings=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_hidden_rnn = num_hidden_rnn\n",
        "        self.num_rnn_layers = num_rnn_layers\n",
        "        self.mlp_layers = mlp_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.embeddings.weight.data.copy_(pretrained_vec) # Load pretrained embeddings\n",
        "        self.embeddings.weight.requires_grad = train_embeddings  # Decide if embeddings get updated or not\n",
        "\n",
        "        # Define RNN layers using \"vanilla\" RNN, GRU or LSTM\n",
        "        if (rec_unit_type == \"RNN\"):\n",
        "            self.rec = nn.RNN(\n",
        "                input_size=self.embed_dim, \n",
        "                hidden_size=self.num_hidden_rnn, \n",
        "                num_layers=num_rnn_layers)\n",
        "        elif (rec_unit_type == \"GRU\"):\n",
        "            self.rec = nn.GRU(\n",
        "                input_size=self.embed_dim, \n",
        "                hidden_size=self.num_hidden_rnn, \n",
        "                num_layers=num_rnn_layers)\n",
        "        elif (rec_unit_type == \"LSTM\"):\n",
        "            self.rec = nn.LSTM(\n",
        "                input_size=self.embed_dim, \n",
        "                hidden_size=self.num_hidden_rnn,\n",
        "                num_layers=num_rnn_layers)\n",
        "\n",
        "        # Define MLP layers\n",
        "        self.mlp = nn.ModuleList()\n",
        "        for i, hidden_layer in enumerate(mlp_layers):\n",
        "            if i == 0:\n",
        "                self.mlp.append(nn.Linear(self.num_hidden_rnn, hidden_layer))\n",
        "            else:\n",
        "                self.mlp.append(nn.Linear(mlp_layers[i-1], hidden_layer))\n",
        "\n",
        "        # Define final output layer\n",
        "        self.out = nn.Linear(mlp_layers[-1], self.num_classes)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # Re-initialize hidden states for each batch, so we are not sharing hidden states across batches\n",
        "        self.h = self.init_hidden(self.num_rnn_layers, X.size(1))\n",
        "        \n",
        "        # Convert words into embeddings\n",
        "        embeddings = self.embeddings(X)\n",
        "        \n",
        "        # Pass through RNN layer and get hidden state at last timestep\n",
        "        output, self.h = self.rec(embeddings, self.h)\n",
        "        \n",
        "        # LSTM returns a tuple of hidden state and cell state, so pick hidden state only\n",
        "        if (rec_unit_type == \"LSTM\"):\n",
        "            self.h = self.h[0]\n",
        "            \n",
        "        # Pass through MLP layer with RELU activation function\n",
        "        out = F.relu(self.mlp[0](self.h[-1]))\n",
        "        for i in range(1, len(self.mlp)):\n",
        "            out = F.relu(self.mlp[i](out))\n",
        "        \n",
        "        return self.out(out)\n",
        "    \n",
        "    def init_hidden(self, num_layers, batch_size): \n",
        "        if (rec_unit_type == \"LSTM\"):\n",
        "            self.h = (Variable(torch.zeros((num_layers,batch_size,self.num_hidden_rnn))).to(device),\n",
        "                      Variable(torch.zeros((num_layers,batch_size,self.num_hidden_rnn))).to(device))\n",
        "        else:\n",
        "            self.h = Variable(torch.zeros((num_layers,batch_size,self.num_hidden_rnn))).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X-lTtQ0yQVc",
        "colab_type": "text"
      },
      "source": [
        "Definition of training loop, where we return the model at the lowest validation loss during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QhZ_myRvuv1B",
        "colab": {}
      },
      "source": [
        "def fit(model, train_dl, val_dl, loss_fn, opt, epochs, batch_size):\n",
        "    measurements = {\n",
        "        \"Train Loss\": [],\n",
        "        \"Train Acc\": [],\n",
        "        \"Val Loss\": [],\n",
        "        \"Val Acc\": [],\n",
        "        \"Lowest Val Loss\": np.Inf,\n",
        "        \"Lowest Val Loss Epoch\": np.Inf\n",
        "    }\n",
        "    lowest_val_loss = np.Inf\n",
        "    lowest_val_loss_epoch = np.Inf\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        y_true_train = list()\n",
        "        y_pred_train = list()\n",
        "        total_loss_train = 0  \n",
        "        \n",
        "        model.train() # Set model in train mode\n",
        "        \n",
        "        for (X,_),y in iter(train_dl):\n",
        "            opt.zero_grad() # Clear old gradients\n",
        "            pred = model(X) # Make prediction using model\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward() # Compute derivatives of loss w.r.t. parameters using backpropagation\n",
        "            opt.step() # Take a step using computed gradients\n",
        "            pred_idx = torch.max(pred, 1)[1]\n",
        "            \n",
        "            y_true_train += list(y.cpu().data.numpy())\n",
        "            y_pred_train += list(pred_idx.cpu().data.numpy())\n",
        "            total_loss_train += loss.item()\n",
        "        \n",
        "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "        train_loss = total_loss_train/len(train_dl)\n",
        "        measurements[\"Train Acc\"].append(train_acc)\n",
        "        measurements[\"Train Loss\"].append(train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        y_true_val = list()\n",
        "        y_pred_val = list()\n",
        "        total_loss_val = 0\n",
        "        \n",
        "        model.eval() # Set model in eval mode\n",
        "\n",
        "        for (X,_),y in iter(val_dl):\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            pred_idx = torch.max(pred, 1)[1]\n",
        "\n",
        "            y_true_val += list(y.cpu().data.numpy())\n",
        "            y_pred_val += list(pred_idx.cpu().data.numpy())\n",
        "            total_loss_val += loss.item()\n",
        "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
        "        val_loss = total_loss_val/len(val_dl)\n",
        "        measurements[\"Val Acc\"].append(val_acc)\n",
        "        measurements[\"Val Loss\"].append(val_loss)\n",
        "        \n",
        "        if (val_loss < measurements[\"Lowest Val Loss\"]):\n",
        "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "            measurements[\"Lowest Val Loss\"] = val_loss\n",
        "            measurements[\"Lowest Val Loss Epoch\"] = epoch\n",
        "        \n",
        "        print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "        \n",
        "    # Return model checkpoint stored at lowest validation loss\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "    print(f'Lowest val_loss: {measurements[\"Lowest Val Loss\"]:.4f}, at epoch {measurements[\"Lowest Val Loss Epoch\"]}')\n",
        "    \n",
        "    return model, measurements"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iMCXvI0PtQjU",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, valds),\n",
        "                                            batch_sizes=(batch_size, batch_size),\n",
        "                                            sort_key=lambda x: len(x.Text),\n",
        "                                            device=device,\n",
        "                                            sort_within_batch=False)\n",
        "train_batch_it = BatchGenerator(traindl, 'Text', 'Label')\n",
        "val_batch_it = BatchGenerator(valdl, 'Text', 'Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLCSo2rQyQVx",
        "colab_type": "text"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTLHOPVqyQVy",
        "colab_type": "text"
      },
      "source": [
        "The hyperparameters that we can tune with this model are:\n",
        "\n",
        "1.   Type of recurrent unit (RNN, GRU, LSTM)\n",
        "2.   Recurrent layer size\n",
        "3.   Depth of recurrent network\n",
        "4.   MLP layer size\n",
        "5.   Depth of MLP\n",
        "6.   Whether to train embeddings\n",
        "\n",
        "To limit the search space via grid-search, we did some preliminary testing to eliminate certain variables and determine the size of the search space for each hyperparameter. We noticed the following:\n",
        "\n",
        "1.   RNN did not achieve validation accuracy comparable to GRU or LSTM under all scenarios.\n",
        "2.   Training embeddings did not help at all.\n",
        "3.   Using anything more than 2 layers of recurrent units or MLP did not improve the validation accuracy. The depth and layer size of MLP did not contribute as much to the performance of the model, compared to the recurrent network.\n",
        "\n",
        "Thus, we decided to conduct grid search with the following hyperparameter search space:\n",
        "\n",
        "1.   Type of recurrent unit: [GRU, LSTM]\n",
        "2.   Recurrent layer size: [30, 40, 50, 60, 70, 80, 100, 150, 200, 250]\n",
        "3.   Depth of recurrent network: [1, 2]\n",
        "4.   MLP layer size: [30, 40, 50, 60, 70]\n",
        "\n",
        "Note: Code below is commented out since this takes a long time to run and only used for exploration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq3mBQtY2Krb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocab_size = len(TEXT.vocab)\n",
        "# embedding_dim = TEXT.vocab.vectors.size()[1]\n",
        "# num_classes = 5\n",
        "# train_embeddings = False\n",
        "\n",
        "# for rec_unit_type in [\"GRU\", \"LSTM\"]:\n",
        "#   for num_hidden_rnn in [30, 40, 50, 60, 70, 80, 100, 150, 200, 250]:\n",
        "#     for num_rnn_layers in [1,2]:\n",
        "#       for mlp_layers in [[30],[40],[50],[60],[70]]:\n",
        "#         print(rec_unit_type, num_hidden_rnn, num_rnn_layers, mlp_layers)\n",
        "\n",
        "#         m = Model(\n",
        "#             vocab_size, \n",
        "#             embedding_dim, \n",
        "#             TEXT.vocab.vectors, \n",
        "#             num_hidden_rnn, \n",
        "#             num_rnn_layers, \n",
        "#             mlp_layers, \n",
        "#             num_classes, \n",
        "#             rec_unit_type, \n",
        "#             train_embeddings).to(device)\n",
        "#         opt = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n",
        "\n",
        "#         m, measurements = fit(\n",
        "#             model=m,\n",
        "#             train_dl=train_batch_it, \n",
        "#             val_dl=val_batch_it, \n",
        "#             loss_fn=F.cross_entropy,\n",
        "#             opt=opt,\n",
        "#             epochs=25,\n",
        "#             batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzC2e40a85EO",
        "colab_type": "text"
      },
      "source": [
        "Looking at the results of grid search, we noticed that:\n",
        "\n",
        "\n",
        "1.   GRU reach the highest validation accuracy (~44%) than LSTM in most scenarios\n",
        "2.   Out of those hyperparameter combinations, a common theme was 2-layer GRUs\n",
        "3.   There were a couple of combinations of parameters that achieved consistently high validation accuracy (~33%). Thus, we selected the combination with the least number of units for generalisation reasons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5P5X5qoBQXM",
        "colab_type": "text"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFjwQOgIiYDW",
        "colab_type": "text"
      },
      "source": [
        "Here, we have our optimum architecture based on our grid search. We trained our model to convergence on training loss. However, we picked the model when it has the lowest validation loss, which happens quite early on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wJODJ5S5tRSk",
        "outputId": "b4dabd2c-0f42-438f-e269-d6fe60d5db9f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
        "num_hidden_rnn = 50\n",
        "num_rnn_layers = 2\n",
        "mlp_layers = [50]\n",
        "num_classes = 5\n",
        "rec_unit_type = \"GRU\"\n",
        "train_embeddings = False\n",
        "\n",
        "m = Model(\n",
        "    vocab_size, \n",
        "    embedding_dim, \n",
        "    TEXT.vocab.vectors, \n",
        "    num_hidden_rnn, \n",
        "    num_rnn_layers, \n",
        "    mlp_layers, \n",
        "    num_classes, \n",
        "    rec_unit_type, \n",
        "    train_embeddings).to(device)\n",
        "# Filter is in place to ensure only layers with \"requires_grad\" will have their weights updated\n",
        "opt = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n",
        "\n",
        "print(m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (embeddings): Embedding(18254, 300)\n",
            "  (rec): GRU(300, 50, num_layers=2)\n",
            "  (mlp): ModuleList(\n",
            "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
            "  )\n",
            "  (out): Linear(in_features=50, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3zrl8-EQ24Wm",
        "outputId": "c639426d-acdc-4520-96d1-0c4d86fe9bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "m, measurements = fit(\n",
        "    model=m,\n",
        "    train_dl=train_batch_it, \n",
        "    val_dl=val_batch_it, \n",
        "    loss_fn=F.cross_entropy,\n",
        "    opt=opt,\n",
        "    epochs=100,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train_loss: 1.5715 train_acc: 0.2665 | val_loss: 1.5787 val_acc: 0.2534\n",
            "Epoch 1: train_loss: 1.5685 train_acc: 0.2704 | val_loss: 1.5682 val_acc: 0.2589\n",
            "Epoch 2: train_loss: 1.5236 train_acc: 0.3150 | val_loss: 1.4061 val_acc: 0.3833\n",
            "Epoch 3: train_loss: 1.3206 train_acc: 0.4084 | val_loss: 1.4621 val_acc: 0.3633\n",
            "Epoch 4: train_loss: 1.2489 train_acc: 0.4426 | val_loss: 1.3289 val_acc: 0.4133\n",
            "Epoch 5: train_loss: 1.2102 train_acc: 0.4544 | val_loss: 1.3418 val_acc: 0.4205\n",
            "Epoch 6: train_loss: 1.1798 train_acc: 0.4753 | val_loss: 1.3229 val_acc: 0.4060\n",
            "Epoch 7: train_loss: 1.1579 train_acc: 0.4798 | val_loss: 1.3177 val_acc: 0.4269\n",
            "Epoch 8: train_loss: 1.1384 train_acc: 0.4925 | val_loss: 1.3349 val_acc: 0.4069\n",
            "Epoch 9: train_loss: 1.1169 train_acc: 0.5049 | val_loss: 1.3678 val_acc: 0.4133\n",
            "Epoch 10: train_loss: 1.0849 train_acc: 0.5254 | val_loss: 1.3218 val_acc: 0.4196\n",
            "Epoch 11: train_loss: 1.0637 train_acc: 0.5296 | val_loss: 1.3211 val_acc: 0.4205\n",
            "Epoch 12: train_loss: 1.0370 train_acc: 0.5500 | val_loss: 1.3192 val_acc: 0.4360\n",
            "Epoch 13: train_loss: 1.0040 train_acc: 0.5611 | val_loss: 1.3326 val_acc: 0.4342\n",
            "Epoch 14: train_loss: 0.9652 train_acc: 0.5879 | val_loss: 1.4055 val_acc: 0.4142\n",
            "Epoch 15: train_loss: 0.9348 train_acc: 0.6044 | val_loss: 1.3449 val_acc: 0.4332\n",
            "Epoch 16: train_loss: 0.8870 train_acc: 0.6264 | val_loss: 1.4176 val_acc: 0.4405\n",
            "Epoch 17: train_loss: 0.8461 train_acc: 0.6553 | val_loss: 1.4696 val_acc: 0.4024\n",
            "Epoch 18: train_loss: 0.7983 train_acc: 0.6737 | val_loss: 1.5255 val_acc: 0.4114\n",
            "Epoch 19: train_loss: 0.7600 train_acc: 0.6963 | val_loss: 1.5124 val_acc: 0.4360\n",
            "Epoch 20: train_loss: 0.7215 train_acc: 0.7143 | val_loss: 1.6241 val_acc: 0.4096\n",
            "Epoch 21: train_loss: 0.6816 train_acc: 0.7335 | val_loss: 1.6068 val_acc: 0.4205\n",
            "Epoch 22: train_loss: 0.6206 train_acc: 0.7612 | val_loss: 1.7095 val_acc: 0.4114\n",
            "Epoch 23: train_loss: 0.5855 train_acc: 0.7787 | val_loss: 1.8020 val_acc: 0.4178\n",
            "Epoch 24: train_loss: 0.5466 train_acc: 0.7946 | val_loss: 1.7930 val_acc: 0.4060\n",
            "Epoch 25: train_loss: 0.5135 train_acc: 0.8075 | val_loss: 1.8339 val_acc: 0.4087\n",
            "Epoch 26: train_loss: 0.4594 train_acc: 0.8332 | val_loss: 1.9529 val_acc: 0.3960\n",
            "Epoch 27: train_loss: 0.4491 train_acc: 0.8391 | val_loss: 1.9812 val_acc: 0.4015\n",
            "Epoch 28: train_loss: 0.4093 train_acc: 0.8533 | val_loss: 2.0102 val_acc: 0.3878\n",
            "Epoch 29: train_loss: 0.4033 train_acc: 0.8548 | val_loss: 1.9791 val_acc: 0.3906\n",
            "Epoch 30: train_loss: 0.3605 train_acc: 0.8741 | val_loss: 2.1139 val_acc: 0.3933\n",
            "Epoch 31: train_loss: 0.3591 train_acc: 0.8735 | val_loss: 2.0893 val_acc: 0.3842\n",
            "Epoch 32: train_loss: 0.3377 train_acc: 0.8814 | val_loss: 2.1528 val_acc: 0.3842\n",
            "Epoch 33: train_loss: 0.3134 train_acc: 0.8917 | val_loss: 2.1821 val_acc: 0.3842\n",
            "Epoch 34: train_loss: 0.2954 train_acc: 0.8985 | val_loss: 2.3144 val_acc: 0.3987\n",
            "Epoch 35: train_loss: 0.2950 train_acc: 0.9007 | val_loss: 2.1946 val_acc: 0.3787\n",
            "Epoch 36: train_loss: 0.2637 train_acc: 0.9123 | val_loss: 2.2931 val_acc: 0.3787\n",
            "Epoch 37: train_loss: 0.2519 train_acc: 0.9165 | val_loss: 2.3980 val_acc: 0.3860\n",
            "Epoch 38: train_loss: 0.2435 train_acc: 0.9204 | val_loss: 2.3956 val_acc: 0.3906\n",
            "Epoch 39: train_loss: 0.2469 train_acc: 0.9167 | val_loss: 2.3734 val_acc: 0.3915\n",
            "Epoch 40: train_loss: 0.2142 train_acc: 0.9313 | val_loss: 2.4698 val_acc: 0.3751\n",
            "Epoch 41: train_loss: 0.2005 train_acc: 0.9353 | val_loss: 2.5304 val_acc: 0.3869\n",
            "Epoch 42: train_loss: 0.2350 train_acc: 0.9223 | val_loss: 2.4547 val_acc: 0.3815\n",
            "Epoch 43: train_loss: 0.2158 train_acc: 0.9279 | val_loss: 2.5196 val_acc: 0.3742\n",
            "Epoch 44: train_loss: 0.1915 train_acc: 0.9398 | val_loss: 2.5594 val_acc: 0.3815\n",
            "Epoch 45: train_loss: 0.1976 train_acc: 0.9349 | val_loss: 2.6282 val_acc: 0.3924\n",
            "Epoch 46: train_loss: 0.1963 train_acc: 0.9368 | val_loss: 2.6185 val_acc: 0.3915\n",
            "Epoch 47: train_loss: 0.1609 train_acc: 0.9489 | val_loss: 2.6694 val_acc: 0.3815\n",
            "Epoch 48: train_loss: 0.1512 train_acc: 0.9535 | val_loss: 2.7694 val_acc: 0.3760\n",
            "Epoch 49: train_loss: 0.1662 train_acc: 0.9486 | val_loss: 2.6611 val_acc: 0.3769\n",
            "Epoch 50: train_loss: 0.1571 train_acc: 0.9501 | val_loss: 2.8423 val_acc: 0.3842\n",
            "Epoch 51: train_loss: 0.1591 train_acc: 0.9492 | val_loss: 2.8139 val_acc: 0.3833\n",
            "Epoch 52: train_loss: 0.1551 train_acc: 0.9513 | val_loss: 2.8339 val_acc: 0.3787\n",
            "Epoch 53: train_loss: 0.1504 train_acc: 0.9528 | val_loss: 2.7870 val_acc: 0.3697\n",
            "Epoch 54: train_loss: 0.1423 train_acc: 0.9542 | val_loss: 2.9722 val_acc: 0.3824\n",
            "Epoch 55: train_loss: 0.1282 train_acc: 0.9603 | val_loss: 2.9595 val_acc: 0.3815\n",
            "Epoch 56: train_loss: 0.1062 train_acc: 0.9685 | val_loss: 3.0172 val_acc: 0.3887\n",
            "Epoch 57: train_loss: 0.1206 train_acc: 0.9621 | val_loss: 3.1832 val_acc: 0.3715\n",
            "Epoch 58: train_loss: 0.1322 train_acc: 0.9585 | val_loss: 3.0752 val_acc: 0.3887\n",
            "Epoch 59: train_loss: 0.1315 train_acc: 0.9599 | val_loss: 3.0779 val_acc: 0.3933\n",
            "Epoch 60: train_loss: 0.1075 train_acc: 0.9689 | val_loss: 3.1196 val_acc: 0.3778\n",
            "Epoch 61: train_loss: 0.0981 train_acc: 0.9707 | val_loss: 3.2232 val_acc: 0.3896\n",
            "Epoch 62: train_loss: 0.1095 train_acc: 0.9650 | val_loss: 3.2339 val_acc: 0.3869\n",
            "Epoch 63: train_loss: 0.1130 train_acc: 0.9644 | val_loss: 3.2577 val_acc: 0.3942\n",
            "Epoch 64: train_loss: 0.1308 train_acc: 0.9615 | val_loss: 3.1922 val_acc: 0.3615\n",
            "Epoch 65: train_loss: 0.1148 train_acc: 0.9648 | val_loss: 3.2305 val_acc: 0.3806\n",
            "Epoch 66: train_loss: 0.1059 train_acc: 0.9675 | val_loss: 3.3129 val_acc: 0.3878\n",
            "Epoch 67: train_loss: 0.1017 train_acc: 0.9676 | val_loss: 3.2458 val_acc: 0.3860\n",
            "Epoch 68: train_loss: 0.0987 train_acc: 0.9693 | val_loss: 3.3746 val_acc: 0.3915\n",
            "Epoch 69: train_loss: 0.0955 train_acc: 0.9707 | val_loss: 3.2927 val_acc: 0.3869\n",
            "Epoch 70: train_loss: 0.0762 train_acc: 0.9767 | val_loss: 3.5887 val_acc: 0.3797\n",
            "Epoch 71: train_loss: 0.0878 train_acc: 0.9725 | val_loss: 3.4288 val_acc: 0.3797\n",
            "Epoch 72: train_loss: 0.1334 train_acc: 0.9588 | val_loss: 3.3572 val_acc: 0.3778\n",
            "Epoch 73: train_loss: 0.0948 train_acc: 0.9716 | val_loss: 3.4578 val_acc: 0.3787\n",
            "Epoch 74: train_loss: 0.0642 train_acc: 0.9817 | val_loss: 3.5601 val_acc: 0.3815\n",
            "Epoch 75: train_loss: 0.0663 train_acc: 0.9815 | val_loss: 3.6235 val_acc: 0.3769\n",
            "Epoch 76: train_loss: 0.0764 train_acc: 0.9786 | val_loss: 3.6059 val_acc: 0.3760\n",
            "Epoch 77: train_loss: 0.0817 train_acc: 0.9758 | val_loss: 3.6564 val_acc: 0.3778\n",
            "Epoch 78: train_loss: 0.0809 train_acc: 0.9745 | val_loss: 3.6749 val_acc: 0.3688\n",
            "Epoch 79: train_loss: 0.0701 train_acc: 0.9785 | val_loss: 3.6845 val_acc: 0.3815\n",
            "Epoch 80: train_loss: 0.1120 train_acc: 0.9634 | val_loss: 3.6674 val_acc: 0.3878\n",
            "Epoch 81: train_loss: 0.0855 train_acc: 0.9754 | val_loss: 3.5565 val_acc: 0.3951\n",
            "Epoch 82: train_loss: 0.0632 train_acc: 0.9821 | val_loss: 3.6144 val_acc: 0.3842\n",
            "Epoch 83: train_loss: 0.0573 train_acc: 0.9836 | val_loss: 3.7583 val_acc: 0.4005\n",
            "Epoch 84: train_loss: 0.0533 train_acc: 0.9855 | val_loss: 3.8373 val_acc: 0.3824\n",
            "Epoch 85: train_loss: 0.0712 train_acc: 0.9792 | val_loss: 3.7866 val_acc: 0.3842\n",
            "Epoch 86: train_loss: 0.0563 train_acc: 0.9850 | val_loss: 3.7946 val_acc: 0.3833\n",
            "Epoch 87: train_loss: 0.0449 train_acc: 0.9870 | val_loss: 3.8795 val_acc: 0.3996\n",
            "Epoch 88: train_loss: 0.0527 train_acc: 0.9846 | val_loss: 3.9318 val_acc: 0.3924\n",
            "Epoch 89: train_loss: 0.0664 train_acc: 0.9789 | val_loss: 3.8572 val_acc: 0.3860\n",
            "Epoch 90: train_loss: 0.0628 train_acc: 0.9808 | val_loss: 3.9508 val_acc: 0.3878\n",
            "Epoch 91: train_loss: 0.0779 train_acc: 0.9768 | val_loss: 3.7715 val_acc: 0.3833\n",
            "Epoch 92: train_loss: 0.0692 train_acc: 0.9799 | val_loss: 3.9271 val_acc: 0.3824\n",
            "Epoch 93: train_loss: 0.0442 train_acc: 0.9878 | val_loss: 4.0137 val_acc: 0.3733\n",
            "Epoch 94: train_loss: 0.0312 train_acc: 0.9917 | val_loss: 4.1155 val_acc: 0.3842\n",
            "Epoch 95: train_loss: 0.0507 train_acc: 0.9854 | val_loss: 4.0678 val_acc: 0.3815\n",
            "Epoch 96: train_loss: 0.0757 train_acc: 0.9774 | val_loss: 4.0531 val_acc: 0.3996\n",
            "Epoch 97: train_loss: 0.0613 train_acc: 0.9823 | val_loss: 4.0071 val_acc: 0.3824\n",
            "Epoch 98: train_loss: 0.0593 train_acc: 0.9830 | val_loss: 4.0772 val_acc: 0.3942\n",
            "Epoch 99: train_loss: 0.0335 train_acc: 0.9904 | val_loss: 4.1749 val_acc: 0.3915\n",
            "Lowest val_loss: 1.3177, at epoch 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrmAmxBlyQV6",
        "colab_type": "code",
        "outputId": "a734078c-884c-42b6-ef5a-32d0bea95cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(measurements[\"Train Loss\"], label=\"Train Loss\")\n",
        "plt.plot(measurements[\"Val Loss\"], label=\"Validation Loss\")\n",
        "plt.axvline(measurements[\"Lowest Val Loss Epoch\"], linestyle='--', color='r',label='Checkpoint')\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2ad013668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAFNCAYAAAAtnkrkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngedyQwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlK2yhmMyQV-",
        "colab_type": "text"
      },
      "source": [
        "## Final Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-g3CyY0t9cqV",
        "colab": {}
      },
      "source": [
        "testdl = data.Iterator(testds,\n",
        "                        batch_size=batch_size,\n",
        "                        sort_key=lambda x: len(x.Text),\n",
        "                        device=device,\n",
        "                        sort_within_batch=False)\n",
        "test_batch_it = BatchGenerator(testdl, 'Text', 'Label')\n",
        "m.eval()\n",
        "loss_fn=F.cross_entropy\n",
        "accuracies = []\n",
        "\n",
        "for batch_it in [train_batch_it, val_batch_it, test_batch_it]:\n",
        "  y_true = list()\n",
        "  y_pred = list()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (X,lengths),y in iter(batch_it):\n",
        "      pred = m(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      pred_idx = torch.max(pred, 1)[1]\n",
        "\n",
        "      y_true += list(y.cpu().data.numpy())\n",
        "      y_pred += list(pred_idx.cpu().data.numpy())\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  accuracies.append(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aEkKa9zyQWF",
        "colab_type": "code",
        "outputId": "de51c6ed-2033-41d0-e654-cb7f7766db2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f'Training Accuracy: {accuracies[0]:.4f}')\n",
        "print(f'Validation Accuracy: {accuracies[1]:.4f}')\n",
        "print(f'Test Accuracy: {accuracies[2]:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5091\n",
            "Validation Accuracy: 0.4269\n",
            "Test Accuracy: 0.4416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFVgJBllyQWH",
        "colab_type": "text"
      },
      "source": [
        "## Test Inferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl54_VloyQWI",
        "colab_type": "code",
        "outputId": "62632557-ca93-468c-a0cc-87d6d3b208e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Movie reviews from:\n",
        "# https://www.rottentomatoes.com/m/the_dark_knight\n",
        "\n",
        "reviews = [\n",
        "    \"An exceptionally smart, brooding picture with some terrific performances.\",\n",
        "    \"You will feel utterly numb after the screening of The Dark Knight. The film is bleak and brilliant.\",\n",
        "    \"The definitive movie of its genre and the best Batman film to date\",\n",
        "    \"Too much psychology and not enough pop. It's possible to be too serious, you know.\",\n",
        "    \"One of the most stylish extravaganzas in years.\",\n",
        "]\n",
        "field = [('Text', TEXT)]\n",
        "my_examples = pd.DataFrame({'text': reviews})\n",
        "my_examples.to_csv('/tmp/my_examples.csv', index=False)\n",
        "my_ds = data.TabularDataset(path='/tmp/my_examples.csv', \n",
        "                            format='csv', \n",
        "                            fields=field, \n",
        "                            skip_header=True)\n",
        "my_dl = data.Iterator(my_ds, batch_size=5, device=device)\n",
        "my_it = BatchGenerator(my_dl, 'Text', 'Text')\n",
        "m.eval()\n",
        "\n",
        "(X,_),_ = next(iter(my_it))\n",
        "pred = m(X)\n",
        "pred_idx = torch.max(pred, 1)[1].cpu().data.numpy()\n",
        "for i in range(len(pred_idx)):\n",
        "  print(\"Sentiment: {}. Review: {}\".format(pred_idx[i], reviews[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: 4. Review: An exceptionally smart, brooding picture with some terrific performances.\n",
            "Sentiment: 4. Review: You will feel utterly numb after the screening of The Dark Knight. The film is bleak and brilliant.\n",
            "Sentiment: 4. Review: The definitive movie of its genre and the best Batman film to date\n",
            "Sentiment: 0. Review: Too much psychology and not enough pop. It's possible to be too serious, you know.\n",
            "Sentiment: 1. Review: One of the most stylish extravaganzas in years.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "MXr25yHvyQWK",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhsYZ_K-yQWL",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8"
      ]
    }
  ]
}